example_id,annotator,batch_id,context,query,gold,baselines_results,is_ill_defined,knowledge_level
1-4330_cca3fb24-9267-4e27-941e-55040f509ca5,annotator1,9c3d1f24-69f6-40b5-8128-3d1b9c13aa51,"There is a growing concern about distinguishing between LLM-generated and human-written texts to prevent misuse, such as the dissemination of misleading information and academic dishonesty. Previous research has primarily focused on classifying text as either entirely human-written or LLM-generated, neglecting the detection of mixed texts that contain both types of content.","In this context, what would be a good source of inspiration for **Distinguishing between llm-generated and human-written texts**?",a token classification problem,"{""ours"": {""suggestion"": ""recent works on machine-generated text detection"", ""k"": ""1"", ""rank"": 3}, ""gpt-4o"": {""suggestion"": ""stylometry techniques"", ""k"": ""1"", ""rank"": 1}, ""random"": {""suggestion"": ""Adversarial attacks against language models(LMs)"", ""k"": ""1"", ""rank"": 4}, ""sciIE"": {""suggestion"": ""LLM's textual representation counterparts"", ""k"": ""1"", ""rank"": 5}, ""mpnet_zero"": {""suggestion"": ""an LLM that effectively processes textual information"", ""k"": ""1"", ""rank"": 6}, ""positive"": {""suggestion"": ""a token classification problem"", ""k"": ""1"", ""rank"": 2}}",,4
1-41666_89a913b3-9f44-4ee4-80d9-4861ab8d754d,annotator1,9c3d1f24-69f6-40b5-8128-3d1b9c13aa51,"The study addresses the challenge of effectively generating AI research leaderboards by extracting specific quadruples from scholarly articles, highlighting the limitations of traditional approaches that rely on predefined taxonomies. It emphasizes the need for improved context selection to enhance the accuracy of large language models and reduce the occurrence of hallucinations in information extraction tasks.","In this context, what would be a good source of inspiration for **The extraction of (task, dataset, metric, score) quadruples from scholarly articles**?",a text generation objective,"{""positive"": {""suggestion"": ""a text generation objective"", ""k"": ""1"", ""rank"": 2}, ""sciIE"": {""suggestion"": ""information extraction tasks"", ""k"": ""1"", ""rank"": 4}, ""random"": {""suggestion"": ""parallel distributed compensation"", ""k"": ""1"", ""rank"": 6}, ""gpt-4o"": {""suggestion"": ""knowledge graph construction techniques"", ""k"": ""1"", ""rank"": 1}, ""ours"": {""suggestion"": ""a standard question-answering task"", ""k"": ""1"", ""rank"": 3}, ""mpnet_zero"": {""suggestion"": ""for the Automatic Term Extraction and Classification (ATE) and Classification) tasks"", ""k"": ""1"", ""rank"": 5}}",,4
1-2288_347a15a9-1b5a-49bf-92a8-a2b9d4cee10d,annotator1,9c3d1f24-69f6-40b5-8128-3d1b9c13aa51,"The abstract highlights issues of inconsistent conceptualization and vague expression in existing NLG quality criteria, which reduce the reliability of LLM evaluations. Additionally, it points out the confusion inherent in LLMs regarding different evaluation criteria, indicating a need for further research and improvements in LLM-based evaluation methods.","In this context, what would be a good source of inspiration for **Nlg evaluation**?",behavioral testing,"{""gpt-4o"": {""suggestion"": ""human linguistic judgment"", ""k"": ""1"", ""rank"": 3}, ""ours"": {""suggestion"": ""human evaluation"", ""k"": ""1"", ""rank"": 1}, ""sciIE"": {""suggestion"": ""LLM-based automatic evaluation metric"", ""k"": ""1"", ""rank"": 4}, ""random"": {""suggestion"": ""implicit feature embeddings"", ""k"": ""1"", ""rank"": 5}, ""mpnet_zero"": {""suggestion"": ""evaluating the quality of text generated by generative Large Language Models(LLMs)"", ""k"": ""1"", ""rank"": 6}, ""positive"": {""suggestion"": ""behavioral testing"", ""k"": ""1"", ""rank"": 2}}",,2
1-22772_2fef663b-19c1-4b0d-b271-d94b993740b1,annotator1,9c3d1f24-69f6-40b5-8128-3d1b9c13aa51,The alignment and coverage of LLM-based evaluations are often limited by the scope and potential bias of the evaluation prompts and criteria. This challenge necessitates a more comprehensive approach to align LLM-based evaluators with human preferences effectively.,"In this context, what would be a good source of inspiration for **Large language models**?",the evaluation mindset of human experts,"{""random"": {""suggestion"": ""Argumentative Components and their corresponding Argumentative Relations (Argumentative Relations)"", ""k"": ""1"", ""rank"": 1}, ""sciIE"": {""suggestion"": ""cross-lingual capabilities of LLMs"", ""k"": ""1"", ""rank"": 2}, ""gpt-4o"": {""suggestion"": ""human cognitive processes"", ""k"": ""1"", ""rank"": 3}, ""mpnet_zero"": {""suggestion"": ""evaluation of large language models"", ""k"": ""1"", ""rank"": 4}, ""ours"": {""suggestion"": ""aligning Large Language Models with human preferences"", ""k"": ""1"", ""rank"": 5}, ""positive"": {""suggestion"": ""the evaluation mindset of human experts"", ""k"": ""1"", ""rank"": 6}}",checked,3
1-5953_75b9e640-b5b5-4843-b993-87357fa3b599,annotator1,9c3d1f24-69f6-40b5-8128-3d1b9c13aa51,Existing large language models (LLMs) underperform in legal judgment prediction due to challenges in understanding case complexities and distinguishing between similar charges. This highlights a need for improved methodologies that can effectively address these issues to enhance judicial efficiency.,"In this context, what would be a good source of inspiration for **Legal judgment prediction**?",human judicial reasoning,"{""sciIE"": {""suggestion"": ""large-scale legal knowledge base"", ""k"": ""1"", ""rank"": 5}, ""mpnet_zero"": {""suggestion"": ""a multitask benchmark dataset for assessing the Arabic legal knowledge of Large Language Models"", ""k"": ""1"", ""rank"": 4}, ""gpt-4o"": {""suggestion"": ""case-based reasoning"", ""k"": ""1"", ""rank"": 2}, ""random"": {""suggestion"": ""Keyphrase Recommendation"", ""k"": ""1"", ""rank"": 6}, ""positive"": {""suggestion"": ""human judicial reasoning"", ""k"": ""1"", ""rank"": 3}, ""ours"": {""suggestion"": ""the Issue, Rule, Application, Conclusion (Issue, Rule, Application, Conclusion) framework, a sequential reasoning approach used by lawyers"", ""k"": ""1"", ""rank"": 1}}",,4
1-447_47f54a5c-2d5a-405b-817f-76d89fc256ae,annotator1,663acfb9-7c58-43e1-b0c6-7b5acca2b14d,"Existing RAG models often treat LLMs as passive recipients of information, which can lead to interference from noisy retrieved content. This approach can result in conflicts between external knowledge and parametric memory, highlighting the need for improved engagement and learning from retrieved evidence.","In this context, what would be a good source of inspiration for **Large language models**?",human learning behavior,"{""ours"": {""suggestion"": ""Anderson's ACT-R (Adaptive Control of Thought-Rational), a cognitive architecture modeling human information access and memory dynamics"", ""k"": ""1"", ""rank"": 1}, ""random"": {""suggestion"": ""a biomedical-specialized pre-trained language model"", ""k"": ""1"", ""rank"": 2}, ""positive"": {""suggestion"": ""human learning behavior"", ""k"": ""1"", ""rank"": 3}, ""sciIE"": {""suggestion"": ""Large-Language model (LLM)"", ""k"": ""1"", ""rank"": 6}, ""gpt-4o"": {""suggestion"": ""interactive learning systems"", ""k"": ""1"", ""rank"": 5}, ""mpnet_zero"": {""suggestion"": ""Large Language Models' knowledge recall mechanisms"", ""k"": ""1"", ""rank"": 4}}",,3
1-31947_b1f88262-1a96-403f-869a-81fc0a4265a7,annotator1,663acfb9-7c58-43e1-b0c6-7b5acca2b14d,"Large language models (LLMs) often struggle to provide up-to-date information due to their one-time training and the constantly evolving nature of the world. Existing approaches to keep LLMs current face difficulties in extracting stored knowledge, highlighting a need for improved methods of knowledge acquisition from raw documents.","In this context, what would be a good source of inspiration for **Improve an llm's ability to effectively acquire new knowledge from raw documents**?",the remarkable success of the Feynman Technique in efficient human learning,"{""gpt-4o"": {""suggestion"": ""neuroscience-inspired memory consolidation"", ""k"": ""1"", ""rank"": 4}, ""ours"": {""suggestion"": ""neuroscience, where the human brain often sheds outdated information to improve the retention of crucial knowledge and facilitate the acquisition of new information"", ""k"": ""1"", ""rank"": 1}, ""sciIE"": {""suggestion"": ""open-source large language models (LLMs)"", ""k"": ""1"", ""rank"": 5}, ""random"": {""suggestion"": ""large foundation models"", ""k"": ""1"", ""rank"": 6}, ""mpnet_zero"": {""suggestion"": ""the fusion between distribution of large language models knowledge and distribution of retrieved texts"", ""k"": ""1"", ""rank"": 3}, ""positive"": {""suggestion"": ""the remarkable success of the Feynman Technique in efficient human learning"", ""k"": ""1"", ""rank"": 2}}",,3
1-641_e3d9632c-90a7-45f0-a4dc-e89908811948,annotator1,663acfb9-7c58-43e1-b0c6-7b5acca2b14d,"Model attribution for LLM-generated disinformation is challenging due to the human-like quality of the disinformation produced and the diversity in prompting methods, which complicates accurate source attribution. An effective attribution model must be invariant to domain-specific features and proficient in identifying originating models across various scenarios, reflecting real-world detection challenges.","In this context, what would be a good source of inspiration for **Model attribution for llm-generated disinformation**?",a domain generalization problem,"{""ours"": {""suggestion"": ""the attribution task"", ""k"": ""1"", ""rank"": 4}, ""mpnet_zero"": {""suggestion"": ""independent disinformation generation characteristics of various large language models"", ""k"": ""1"", ""rank"": 3}, ""sciIE"": {""suggestion"": ""Detecting Misinformation by Integrating Intent featuRes (DM-INTER)"", ""k"": ""1"", ""rank"": 1}, ""positive"": {""suggestion"": ""a domain generalization problem"", ""k"": ""1"", ""rank"": 5}, ""random"": {""suggestion"": ""a self-evolving framework"", ""k"": ""1"", ""rank"": 6}, ""gpt-4o"": {""suggestion"": ""forensic linguistics"", ""k"": ""1"", ""rank"": 2}}",,2
1-23261_dcdf3bfd-5fb5-4416-bf14-0992a328cd59,annotator1,663acfb9-7c58-43e1-b0c6-7b5acca2b14d,"Modern Large Language Models (LLMs) exhibit a performance gap in basic tasks like relation and event extraction, primarily due to the imprecision of existing evaluation metrics and the incompleteness of evaluation benchmarks caused by restrictive human annotation schemas. These issues lead to an underestimation of LLM performances and highlight the need for improved evaluation methods that can better assess semantic consistency and address benchmark limitations.","In this context, what would be a good source of inspiration for **Evaluation of large language models**?",the principles in subjective question correction,"{""gpt-4o"": {""suggestion"": ""human cognitive evaluation"", ""k"": ""1"", ""rank"": 3}, ""mpnet_zero"": {""suggestion"": ""the success of large language models in NLP"", ""k"": ""1"", ""rank"": 1}, ""sciIE"": {""suggestion"": ""fine-tuned large language models (LLMs)"", ""k"": ""1"", ""rank"": 2}, ""positive"": {""suggestion"": ""the principles in subjective question correction"", ""k"": ""1"", ""rank"": 6}, ""ours"": {""suggestion"": ""human evaluation"", ""k"": ""1"", ""rank"": 4}, ""random"": {""suggestion"": ""human flexibility and reasoning"", ""k"": ""1"", ""rank"": 5}}",,4
1-30440_f6e4517a-c74e-4884-92e4-b6df15fcb40b,annotator1,663acfb9-7c58-43e1-b0c6-7b5acca2b14d,"Large Language Models (LLMs) face challenges due to their overreliance on potentially flawed parametric knowledge, leading to hallucinations and inaccuracies, especially with long-tail, domain-specific queries. Additionally, the presence of noisy and irrelevant information in retrieved long-context documents can dilute LLMs' attention, highlighting the need for improved methods to enhance their contextual awareness and robustness.","In this context, what would be a good source of inspiration for **Improve the robustness and contextual awareness of large language models**?",the supportive role of essential concepts in individuals' reading comprehension,"{""random"": {""suggestion"": ""patient monitoring"", ""k"": ""1"", ""rank"": 6}, ""gpt-4o"": {""suggestion"": ""human cognitive processes"", ""k"": ""1"", ""rank"": 3}, ""ours"": {""suggestion"": ""the fusion between distribution of large language models knowledge and distribution of retrieved texts"", ""k"": ""1"", ""rank"": 2}, ""mpnet_zero"": {""suggestion"": ""the problem faced by Large Language Models"", ""k"": ""1"", ""rank"": 4}, ""sciIE"": {""suggestion"": ""Large-Language model (LLM)"", ""k"": ""1"", ""rank"": 5}, ""positive"": {""suggestion"": ""the supportive role of essential concepts in individuals' reading comprehension"", ""k"": ""1"", ""rank"": 1}}",,3
1-848_5134a364-f0a5-4ab3-bee7-78f5387b88db,annotator1,9afc373b-86b0-4b10-b9c0-7cd5721f7085,"The confidence calibration of large language models (LLMs) in response to prompts has not been thoroughly explored, despite the significant impact that prompting strategies have on their performance. Existing methods may improve expected calibration but can also lead to over-confidence in certain instances, indicating a need for better approaches to enhance LLM reliability.","In this context, what would be a good source of inspiration for **Llm**?",human cognition,"{""mpnet_zero"": {""suggestion"": ""existing calibration methods for large language models"", ""k"": ""1"", ""rank"": 1}, ""positive"": {""suggestion"": ""human cognition"", ""k"": ""1"", ""rank"": 2}, ""random"": {""suggestion"": ""traditional backend optimization methods"", ""k"": ""1"", ""rank"": 3}, ""sciIE"": {""suggestion"": ""zero-shot prompting of large language models (LLMs)"", ""k"": ""1"", ""rank"": 4}, ""gpt-4o"": {""suggestion"": ""human metacognition"", ""k"": ""1"", ""rank"": 5}, ""ours"": {""suggestion"": ""uncertainty estimation literature in large language models"", ""k"": ""1"", ""rank"": 6}}",checked,3
1-37730_6df467e6-c5d7-4a07-9565-99147fcadcdb,annotator1,9afc373b-86b0-4b10-b9c0-7cd5721f7085,"Existing methods for temporal relation extraction struggle with limited and unevenly distributed annotated data, highlighting a need for more effective approaches to understand task requests in crowdsourcing systems. This gap in research necessitates innovative solutions that can leverage abundant global knowledge to enhance performance in this area.","In this context, what would be a good source of inspiration for **Temporal relation extraction**?",the abundant global knowledge stored within pre-trained language models,"{""sciIE"": {""suggestion"": ""relation extraction"", ""k"": ""1"", ""rank"": 6}, ""gpt-4o"": {""suggestion"": ""transfer learning"", ""k"": ""1"", ""rank"": 3}, ""positive"": {""suggestion"": ""the abundant global knowledge stored within pre-trained language models"", ""k"": ""1"", ""rank"": 1}, ""mpnet_zero"": {""suggestion"": ""Relation extraction models"", ""k"": ""1"", ""rank"": 5}, ""ours"": {""suggestion"": ""Question answering over heterogeneous data"", ""k"": ""1"", ""rank"": 2}, ""random"": {""suggestion"": ""a supervised contrastive loss"", ""k"": ""1"", ""rank"": 4}}",,4
1-42304_544e6956-8092-4102-8996-c9d4a7500a75,annotator1,9afc373b-86b0-4b10-b9c0-7cd5721f7085,"The performance of large language models (LLMs) can be significantly enhanced through effective prompt optimization, yet existing methods may not fully leverage the potential of LLMs as prompt optimizers. There is a need for improved strategies that systematically analyze and refine task prompts to achieve better outcomes in various benchmarks.","In this context, what would be a good source of inspiration for **Llm-based prompt optimizers**?",gradient-based model optimizers,"{""gpt-4o"": {""suggestion"": ""genetic algorithms"", ""k"": ""1"", ""rank"": 2}, ""ours"": {""suggestion"": ""the concept of prompt tuning"", ""k"": ""1"", ""rank"": 5}, ""mpnet_zero"": {""suggestion"": ""prompt-engineering-based large language models"", ""k"": ""1"", ""rank"": 4}, ""random"": {""suggestion"": ""Learning effective representations from raw data"", ""k"": ""1"", ""rank"": 6}, ""positive"": {""suggestion"": ""gradient-based model optimizers"", ""k"": ""1"", ""rank"": 1}, ""sciIE"": {""suggestion"": ""prompt-engineering-based large language models (LLMs)"", ""k"": ""1"", ""rank"": 3}}",,3
1-23981_af245886-71d0-41f7-80a0-f0df90e5ec51,annotator1,9afc373b-86b0-4b10-b9c0-7cd5721f7085,"The ability of large language models (LLMs) to distinguish subtle sentiments remains a challenge, indicating a need for improved methods in sentiment analysis. Existing in-context learning (ICL) approaches may not adequately address sentiment misinterpretation, highlighting a gap in the effectiveness of current techniques.","In this context, what would be a good source of inspiration for **Large language models**?",the human ability to adjust understanding via feedback,"{""random"": {""suggestion"": ""spatial features"", ""k"": ""1"", ""rank"": 1}, ""gpt-4o"": {""suggestion"": ""human emotional intelligence"", ""k"": ""1"", ""rank"": 2}, ""ours"": {""suggestion"": ""in-context learning in natural language processing"", ""k"": ""1"", ""rank"": 3}, ""sciIE"": {""suggestion"": ""fine-tuned large language models (LLMs)"", ""k"": ""1"", ""rank"": 4}, ""mpnet_zero"": {""suggestion"": ""a sentiment analysis model using local large language models"", ""k"": ""1"", ""rank"": 5}, ""positive"": {""suggestion"": ""the human ability to adjust understanding via feedback"", ""k"": ""1"", ""rank"": 6}}",checked,3
1-34117_868de9d7-8184-4bfc-b655-a3df1c2c4980,annotator1,9afc373b-86b0-4b10-b9c0-7cd5721f7085,"Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging due to the need to verify both adherence to instructions and the grounding of text outputs in the provided images. This highlights a gap in effective evaluation methods for VLMs, necessitating innovative approaches to improve assessment accuracy and transparency.","In this context, what would be a good source of inspiration for **Evaluating vision-language models**?",evaluating LMs with LMs,"{""gpt-4o"": {""suggestion"": ""human visual perception"", ""k"": ""1"", ""rank"": 4}, ""positive"": {""suggestion"": ""evaluating LMs with LMs"", ""k"": ""1"", ""rank"": 2}, ""random"": {""suggestion"": ""the parameters trained with gradient descent"", ""k"": ""1"", ""rank"": 6}, ""ours"": {""suggestion"": ""human evaluation"", ""k"": ""1"", ""rank"": 3}, ""sciIE"": {""suggestion"": ""multimodal vision-language models (VLMs)"", ""k"": ""1"", ""rank"": 5}, ""mpnet_zero"": {""suggestion"": ""evaluate high-level cognitive ability of Large Vision-Language Models using images with rich semantics"", ""k"": ""1"", ""rank"": 1}}",,2
1-848_8cf46458-3b1a-47b0-b819-4210de88aa61,annotator1,8599401e-f3f4-403f-bb15-985dd8cfad2b,"Existing literature has predominantly focused on ingroup favoritism, often overlooking outgroup bias, which is a fundamental source of intergroup prejudice and discrimination. This gap in research highlights the need to explore how identity assignments in large language models can lead to both ingroup favoritism and outgroup bias.","In this context, what would be a good source of inspiration for **Artificial intelligence**?",human cognition,"{""positive"": {""suggestion"": ""human cognition"", ""k"": ""1"", ""rank"": 1}, ""random"": {""suggestion"": ""item's correlation and sequential information from the search system to build a heterogeneous graph for better Click-through-rate prediction in e-commerce search"", ""k"": ""1"", ""rank"": 2}, ""ours"": {""suggestion"": ""social philosophy"", ""k"": ""1"", ""rank"": 3}, ""sciIE"": {""suggestion"": ""inter-identity discrimination"", ""k"": ""1"", ""rank"": 4}, ""mpnet_zero"": {""suggestion"": ""modeling the intergroup bias"", ""k"": ""1"", ""rank"": 5}, ""gpt-4o"": {""suggestion"": ""social identity theory"", ""k"": ""1"", ""rank"": 6}}",checked,3
1-40473_92341f82-89ed-4e50-a21c-e848cddb91c1,annotator1,8599401e-f3f4-403f-bb15-985dd8cfad2b,Reasoning about compositional rules is challenging because it requires multiple reasoning steps and attending to the logical relationships between elements. There is a need for effective methods to elicit rule-based reasoning in complex logical expressions.,"In this context, what would be a good source of inspiration for **Causal language models as rule-based reasoners**?","the Issue, Rule, Application, Conclusion (Issue, Rule, Application, Conclusion) framework, a sequential reasoning approach used by lawyers","{""sciIE"": {""suggestion"": ""rule-based reasoners"", ""k"": ""1"", ""rank"": 5}, ""random"": {""suggestion"": ""unitary weights"", ""k"": ""1"", ""rank"": 6}, ""gpt-4o"": {""suggestion"": ""symbolic logic systems"", ""k"": ""1"", ""rank"": 3}, ""mpnet_zero"": {""suggestion"": ""rule-based logic"", ""k"": ""1"", ""rank"": 4}, ""positive"": {""suggestion"": ""the Issue, Rule, Application, Conclusion (Issue, Rule, Application, Conclusion) framework, a sequential reasoning approach used by lawyers"", ""k"": ""1"", ""rank"": 1}, ""ours"": {""suggestion"": ""Pearl's structural causal models"", ""k"": ""1"", ""rank"": 2}}",,2
1-710_6debd94c-e2b4-4874-a585-24876952adbb,annotator1,8599401e-f3f4-403f-bb15-985dd8cfad2b,"The propagation of social biases within large language models, inherited from diverse training datasets, presents a significant challenge in understanding and mitigating these biases. There is a necessity for tailored debiasing strategies and a deeper understanding of the complex mechanisms and pathways through which bias operates in these models.","In this context, what would be a good source of inspiration for **The evolution of bias-related features in large language models**?",causal mediation analysis,"{""random"": {""suggestion"": ""a Lagrangian-mechanics-based physical model"", ""k"": ""1"", ""rank"": 6}, ""gpt-4o"": {""suggestion"": ""cultural evolution theory"", ""k"": ""1"", ""rank"": 2}, ""mpnet_zero"": {""suggestion"": ""studying biases and inherent knowledge of large language modelss"", ""k"": ""1"", ""rank"": 3}, ""sciIE"": {""suggestion"": ""Large Language Model Bias Index (LLMBI)"", ""k"": ""1"", ""rank"": 4}, ""positive"": {""suggestion"": ""causal mediation analysis"", ""k"": ""1"", ""rank"": 5}, ""ours"": {""suggestion"": ""the spread of rumors or influence in an online social network"", ""k"": ""1"", ""rank"": 1}}",,2
1-31870_974857e0-abd8-4ef1-8ef5-2ef3a634c21f,annotator1,8599401e-f3f4-403f-bb15-985dd8cfad2b,"The internal mechanisms of how multimodal large language models process features from diverse domains remain unexplored, indicating a need for further investigation into the distribution of domain-specific neurons. Additionally, while current models demonstrate Visual Question Answering capability, they do not fully utilize domain-specific information, highlighting a gap in their effectiveness.","In this context, what would be a good source of inspiration for **Projecting visual features into word embedding space**?",multilingual research,"{""positive"": {""suggestion"": ""multilingual research"", ""k"": ""1"", ""rank"": 2}, ""sciIE"": {""suggestion"": ""multimodal large language model"", ""k"": ""1"", ""rank"": 5}, ""random"": {""suggestion"": ""image-based generative AI"", ""k"": ""1"", ""rank"": 3}, ""gpt-4o"": {""suggestion"": ""multimodal transformers"", ""k"": ""1"", ""rank"": 6}, ""mpnet_zero"": {""suggestion"": ""state-of-the-art multimodal large language models"", ""k"": ""1"", ""rank"": 4}, ""ours"": {""suggestion"": ""the image-to-text mapping process by the multimodal connector"", ""k"": ""1"", ""rank"": 1}}",,2
1-7666_2a8784a6-8ba4-427a-83c8-f512ec6d752c,annotator1,8599401e-f3f4-403f-bb15-985dd8cfad2b,"Existing methods for early exiting in large language models require significant effort to train internal classifiers and can only achieve comparable performance at best, highlighting a need for more efficient approaches. Additionally, the high computational overhead associated with model inference presents a challenge that necessitates innovative solutions to accelerate inference while maintaining performance.","In this context, what would be a good source of inspiration for **The early exiting problem**?",a distribution prediction problem,"{""ours"": {""suggestion"": ""a sequential decision-making problem"", ""k"": ""1"", ""rank"": 2}, ""gpt-4o"": {""suggestion"": ""dynamic neural networks"", ""k"": ""1"", ""rank"": 3}, ""sciIE"": {""suggestion"": ""few-shot capabilities of large language models"", ""k"": ""1"", ""rank"": 1}, ""random"": {""suggestion"": ""Vision-Language foundation models (VL-models)"", ""k"": ""1"", ""rank"": 5}, ""mpnet_zero"": {""suggestion"": ""recent success of Large Language Models"", ""k"": ""1"", ""rank"": 6}, ""positive"": {""suggestion"": ""a distribution prediction problem"", ""k"": ""1"", ""rank"": 4}}",,2
1-11323_c67c7988-f949-4d36-bbbc-8c0c132d9a4c,annotator1,36c6fab5-add9-4de6-98a7-458238a50e5a,"Aligning language models with human preferences is crucial for better meeting diverse user needs, and there is a need to effectively transfer alignment behavior from weaker models to stronger ones. The observation that stronger models can benefit from the alignment capabilities of weaker models highlights a gap in existing approaches to model alignment.","In this context, what would be a good source of inspiration for **Model alignment**?",weak-to-strong generalization,"{""mpnet_zero"": {""suggestion"": ""the language model alignment"", ""k"": ""1"", ""rank"": 2}, ""gpt-4o"": {""suggestion"": ""human collaborative learning"", ""k"": ""1"", ""rank"": 3}, ""sciIE"": {""suggestion"": ""language model fine-tuning techniques"", ""k"": ""1"", ""rank"": 5}, ""positive"": {""suggestion"": ""weak-to-strong generalization"", ""k"": ""1"", ""rank"": 4}, ""random"": {""suggestion"": ""partitioning large-scale hypergraphs"", ""k"": ""1"", ""rank"": 6}, ""ours"": {""suggestion"": ""aligning Large Language Models with human preferences"", ""k"": ""1"", ""rank"": 1}}",,3
1-37407_dea6d3d7-f9e5-4d01-8685-22b3b9a2e164,annotator1,36c6fab5-add9-4de6-98a7-458238a50e5a,"The study identifies deficiencies in reference-free metrics used for evaluating the compatibility between textual descriptions and images, highlighting issues such as incoherent statements and excessive repetition in generated descriptions. This indicates a need for improved evaluation methods that can better align with human judgment and rectify the shortcomings of existing metrics.","In this context, what would be a good source of inspiration for **Evaluating the compatibility between textual descriptions and corresponding images**?",the Cobra Effect,"{""ours"": {""suggestion"": ""evaluating the quality of text generated by generative Large Language Models(LLMs)"", ""k"": ""1"", ""rank"": 5}, ""sciIE"": {""suggestion"": ""low consistency between image and text descriptions"", ""k"": ""1"", ""rank"": 3}, ""random"": {""suggestion"": ""the image-text mapping problem"", ""k"": ""1"", ""rank"": 1}, ""mpnet_zero"": {""suggestion"": ""a text-to-image alignment quality prediction task"", ""k"": ""1"", ""rank"": 4}, ""positive"": {""suggestion"": ""the Cobra Effect"", ""k"": ""1"", ""rank"": 2}, ""gpt-4o"": {""suggestion"": ""human visual perception"", ""k"": ""1"", ""rank"": 6}}",,3
1-38849_3c50b338-017d-4f6a-8159-dec2b56342d4,annotator1,36c6fab5-add9-4de6-98a7-458238a50e5a,"Previous works mostly focus on generating better responses but ignore interpretability, which is extremely important for constructing reliable dialogue systems. There is a need to empower systems with better interpretability to enhance the understanding of emotional support responses and establish connections between users and dialogue systems.","In this context, what would be a good source of inspiration for **Empower the system with better interpretability**?","the process of identifying, understanding, and regulating emotions","{""positive"": {""suggestion"": ""the process of identifying, understanding, and regulating emotions"", ""k"": ""1"", ""rank"": 1}, ""gpt-4o"": {""suggestion"": ""cognitive psychology"", ""k"": ""1"", ""rank"": 4}, ""mpnet_zero"": {""suggestion"": ""Emotion Support Conversation"", ""k"": ""1"", ""rank"": 3}, ""ours"": {""suggestion"": ""foundational principles of human communication within psychology"", ""k"": ""1"", ""rank"": 2}, ""random"": {""suggestion"": ""two branch paths from two different multi-scale approaches"", ""k"": ""1"", ""rank"": 5}, ""sciIE"": {""suggestion"": ""language-based interpretability"", ""k"": ""1"", ""rank"": 6}}",,3
1-28499_a7ef1baf-ab5c-453f-915e-f7d3429a8017,annotator1,36c6fab5-add9-4de6-98a7-458238a50e5a,"To enhance zero-shot translation, models need to share knowledge across languages, which is a challenge in many-to-many multilingual neural machine translation. Existing methods may not effectively leverage both semantic and linguistic features, limiting their ability to improve translation performance across diverse languages.","In this context, what would be a good source of inspiration for **The many-to-many multilingual neural machine translation**?",the process of integrating semantic features from the source sentences and linguistic features from the target sentences,"{""ours"": {""suggestion"": ""quantifying the alignment and overlap of concepts across languages in multilingual embeddings"", ""k"": ""1"", ""rank"": 1}, ""mpnet_zero"": {""suggestion"": ""Multilingual neural machine translation models"", ""k"": ""1"", ""rank"": 3}, ""positive"": {""suggestion"": ""the process of integrating semantic features from the source sentences and linguistic features from the target sentences"", ""k"": ""1"", ""rank"": 2}, ""sciIE"": {""suggestion"": ""Pre-trained sequence-to-sequence (seq2seq) multi-lingual models"", ""k"": ""1"", ""rank"": 4}, ""random"": {""suggestion"": ""cognitive load theory"", ""k"": ""1"", ""rank"": 6}, ""gpt-4o"": {""suggestion"": ""multilingual pretraining"", ""k"": ""1"", ""rank"": 5}}",,3
1-22875_9e64c8a0-a8f1-438a-b177-a11aa8e6a2a3,annotator1,36c6fab5-add9-4de6-98a7-458238a50e5a,"Existing methods for Emotion-Cause Pair Extraction tend to overfit spurious correlations, such as positional bias in benchmark datasets, rather than effectively capturing semantic features. Additionally, while large language models have strong capabilities, they suffer from uncontrollable outputs, leading to mediocre performance in this task.","In this context, what would be a good source of inspiration for **Emotion-cause pair extraction**?",recent work,"{""ours"": {""suggestion"": ""a dependency-sensitive language-modeling problem"", ""k"": ""1"", ""rank"": 4}, ""random"": {""suggestion"": ""a generic convolution path"", ""k"": ""1"", ""rank"": 5}, ""gpt-4o"": {""suggestion"": ""cognitive psychology insights"", ""k"": ""1"", ""rank"": 3}, ""positive"": {""suggestion"": ""recent work"", ""k"": ""1"", ""rank"": 6}, ""mpnet_zero"": {""suggestion"": ""Multimodal emotion recognition in conversation and multimodal emotion-cause pair extraction (multimodal emotion-cause pair extraction)"", ""k"": ""1"", ""rank"": 2}, ""sciIE"": {""suggestion"": ""multimodal emotion-cause pair extraction (MECPE)"", ""k"": ""1"", ""rank"": 1}}",,2
1-35139_06200ab8-f2d8-408d-9232-4f2f29c9e2d7,annotator1,dfb919a3-d6c4-45ca-8c6d-299d811c81a6,"The complex nature of clinical environments presents significant hallucination challenges for large language models (LLMs), hindering their widespread adoption in medical applications. Addressing these hallucination issues is crucial for improving the effectiveness of Medical Information Extraction tasks.","In this context, what would be a good source of inspiration for **Medical information extraction tasks**?",an identify-and-classify process,"{""mpnet_zero"": {""suggestion"": ""developing large language models in the medical domain to assist clinicians"", ""k"": ""1"", ""rank"": 5}, ""positive"": {""suggestion"": ""an identify-and-classify process"", ""k"": ""1"", ""rank"": 3}, ""sciIE"": {""suggestion"": ""complexity of medical language"", ""k"": ""1"", ""rank"": 4}, ""random"": {""suggestion"": ""mechanical simulation using the finite element method"", ""k"": ""1"", ""rank"": 6}, ""gpt-4o"": {""suggestion"": ""knowledge distillation techniques"", ""k"": ""1"", ""rank"": 2}, ""ours"": {""suggestion"": ""formulate an EHR question-answering task"", ""k"": ""1"", ""rank"": 1}}",,4
1-41549_fe8b430d-566c-4f49-b4d5-49035a791a6d,annotator1,dfb919a3-d6c4-45ca-8c6d-299d811c81a6,"The evaluation of large language models (LLMs) for clinical applications is critical to ensure their safe and reliable use, particularly in mitigating potential risks such as hallucinations. Current evaluation methods are heavily reliant on labor-intensive human participation, highlighting the need for more efficient and automated approaches to assess LLMs' capabilities in medical diagnosis and treatment.","In this context, what would be a good source of inspiration for **An automatic evaluation paradigm tailored to assess the large language models' capabilities in delivering clinical services, e.g., disease diagnosis and treatment**?",professional clinical practice pathways,"{""gpt-4o"": {""suggestion"": ""simulation-based testing frameworks"", ""k"": ""1"", ""rank"": 2}, ""mpnet_zero"": {""suggestion"": ""consulting a large language model and medical experts"", ""k"": ""1"", ""rank"": 3}, ""sciIE"": {""suggestion"": ""complexity of medical language"", ""k"": ""1"", ""rank"": 6}, ""positive"": {""suggestion"": ""professional clinical practice pathways"", ""k"": ""1"", ""rank"": 1}, ""random"": {""suggestion"": ""task-specific NLP models"", ""k"": ""1"", ""rank"": 5}, ""ours"": {""suggestion"": ""the widely-adopted \""needle-in-a-haystack\"" (needle-in-a-haystack\"") evaluation"", ""k"": ""1"", ""rank"": 4}}",,2
1-30045_e2830c52-6a9e-484e-afb0-191398e9b81b,annotator1,dfb919a3-d6c4-45ca-8c6d-299d811c81a6,"The reasoning ability of large language models (LLMs) is limited in mathematical reasoning, which is crucial for various fields such as healthcare, transport, and aerospace. There is a significant need to enhance the mathematical reasoning capabilities of LLMs to address this gap and improve their performance in solving mathematical problems.","In this context, what would be a good source of inspiration for **Improve the mathematical reasoning ability of large language models**?",the human learning process of generalization of learned information and effective application of previous knowledge in new reasoning tasks,"{""gpt-4o"": {""suggestion"": ""symbolic reasoning techniques"", ""k"": ""1"", ""rank"": 2}, ""sciIE"": {""suggestion"": ""Large-Language model (LLM)"", ""k"": ""1"", ""rank"": 5}, ""random"": {""suggestion"": ""self-distillation"", ""k"": ""1"", ""rank"": 6}, ""ours"": {""suggestion"": ""the cognitive mechanism in human mathematical learning"", ""k"": ""1"", ""rank"": 3}, ""mpnet_zero"": {""suggestion"": ""improve the reasoning capabilities of Large Language Models"", ""k"": ""1"", ""rank"": 4}, ""positive"": {""suggestion"": ""the human learning process of generalization of learned information and effective application of previous knowledge in new reasoning tasks"", ""k"": ""1"", ""rank"": 1}}",,3
1-25013_fa771011-68c5-4f3c-8724-0884e85b14e9,annotator1,dfb919a3-d6c4-45ca-8c6d-299d811c81a6,"The performance of large language models on mathematical problems and reasoning tasks is limited due to the inherent difficulty of these problems and the multi-step nature of their solutions, which complicates the effectiveness of a single prompting technique. This highlights a need for improved approaches that can guide LLMs through varying cognitive processes to reach correct solutions.","In this context, what would be a good source of inspiration for **A new prompting technique**?",Bloom's Taxonomy,"{""positive"": {""suggestion"": ""Bloom's Taxonomy"", ""k"": ""1"", ""rank"": 1}, ""random"": {""suggestion"": ""static background priors"", ""k"": ""1"", ""rank"": 6}, ""ours"": {""suggestion"": ""well-developed cognitive science"", ""k"": ""1"", ""rank"": 4}, ""sciIE"": {""suggestion"": ""prompt-engineering-based large language models (LLMs)"", ""k"": ""1"", ""rank"": 3}, ""mpnet_zero"": {""suggestion"": ""text prompting in Large Language Models"", ""k"": ""1"", ""rank"": 5}, ""gpt-4o"": {""suggestion"": ""cognitive task analysis"", ""k"": ""1"", ""rank"": 2}}",,3
1-31809_e5ffa1fb-0958-4eeb-a7e1-a0efae1ae46f,annotator1,dfb919a3-d6c4-45ca-8c6d-299d811c81a6,"The task of finding plausible knowledge missing from a given ontology is highly challenging, particularly for Large Language Models, even after fine-tuning. Additionally, there is a need to evaluate and compare different approaches to ontology completion, as previous research has not thoroughly analyzed their effectiveness or the potential for hybrid strategies.","In this context, what would be a good source of inspiration for **The problem of finding plausible knowledge that is missing from a given ontology**?",a generalisation of the well-studied taxonomy expansion task,"{""gpt-4o"": {""suggestion"": ""knowledge graph embeddings"", ""k"": ""1"", ""rank"": 2}, ""ours"": {""suggestion"": ""a knowledge graph completion problem"", ""k"": ""1"", ""rank"": 1}, ""random"": {""suggestion"": ""nodes in a bipartite graph that is fully connected"", ""k"": ""1"", ""rank"": 6}, ""positive"": {""suggestion"": ""a generalisation of the well-studied taxonomy expansion task"", ""k"": ""1"", ""rank"": 3}, ""mpnet_zero"": {""suggestion"": ""ontological knowledge"", ""k"": ""1"", ""rank"": 5}, ""sciIE"": {""suggestion"": ""Ontology Learning"", ""k"": ""1"", ""rank"": 4}}",,3
1-27849_0140b3af-bfaf-4266-b40e-f89d235de460,annotator1,0ff39183-ed81-403d-b6ed-fc158fe2227f,"The evaluation of LLM-based Emotion Support Conversations (ESCs) remains uncertain, highlighting a need for a systematic approach to assess their effectiveness. Additionally, there is a notable gap in performance between ESC-oriented LLMs and human capabilities, indicating a need for improved evaluation frameworks.","In this context, what would be a good source of inspiration for **Emotion support conversation**?",the awesome development of role-playing agents,"{""random"": {""suggestion"": ""traditional task tokens"", ""k"": ""1"", ""rank"": 4}, ""mpnet_zero"": {""suggestion"": ""a Textual Conversational Interface, powered by tool-augmented LLM"", ""k"": ""1"", ""rank"": 5}, ""gpt-4o"": {""suggestion"": ""human empathy"", ""k"": ""1"", ""rank"": 3}, ""positive"": {""suggestion"": ""the awesome development of role-playing agents"", ""k"": ""1"", ""rank"": 1}, ""ours"": {""suggestion"": ""human evaluation"", ""k"": ""1"", ""rank"": 6}, ""sciIE"": {""suggestion"": ""affective-linguistic communication"", ""k"": ""1"", ""rank"": 2}}",checked,3
1-40766_f1a0dc30-65c4-49c6-b3a5-659c27e6fc78,annotator1,0ff39183-ed81-403d-b6ed-fc158fe2227f,"The application of large language models (LLMs) as evaluators in pairwise comparisons often leads to selection bias, resulting in inconsistent judgments that compromise the fairness and effectiveness of evaluation results. This highlights a significant challenge in developing reliable automated evaluation frameworks that can mitigate such biases and improve performance in AI-driven assessments.","In this context, what would be a good source of inspiration for **Debiasing**?",an optimization task aimed at adjusting observed prediction distributions to align with unbiased prediction distributions,"{""ours"": {""suggestion"": ""bias examination"", ""k"": ""1"", ""rank"": 5}, ""gpt-4o"": {""suggestion"": ""adversarial training"", ""k"": ""1"", ""rank"": 2}, ""mpnet_zero"": {""suggestion"": ""Large Language Model Bias Index"", ""k"": ""1"", ""rank"": 4}, ""positive"": {""suggestion"": ""an optimization task aimed at adjusting observed prediction distributions to align with unbiased prediction distributions"", ""k"": ""1"", ""rank"": 1}, ""sciIE"": {""suggestion"": ""Large Language Model Bias Index (LLMBI)"", ""k"": ""1"", ""rank"": 3}, ""random"": {""suggestion"": ""a human-written document"", ""k"": ""1"", ""rank"": 6}}",,2
1-15422_c766cd0d-6d57-4be5-8d8c-a255c127fea9,annotator1,0ff39183-ed81-403d-b6ed-fc158fe2227f,Existing methods for generating rationales in automated scoring systems do not match the accuracy of classifier-based methods and often produce hallucinated information. There is a need for a framework that can generate more faithful rationales while achieving performance comparable to black-box scoring systems.,"In this context, what would be a good source of inspiration for **Automated scoring systems**?",the human assessment process,"{""random"": {""suggestion"": ""examine inferential strategies employed by large language models, through a detailed evaluation of their responses to propositional logic problems"", ""k"": ""1"", ""rank"": 1}, ""gpt-4o"": {""suggestion"": ""explainable AI techniques"", ""k"": ""1"", ""rank"": 2}, ""ours"": {""suggestion"": ""recent work on knowledge generation from LLMs for text-based QA"", ""k"": ""1"", ""rank"": 3}, ""mpnet_zero"": {""suggestion"": ""model-generated scores"", ""k"": ""1"", ""rank"": 4}, ""sciIE"": {""suggestion"": ""synthetic rationale data"", ""k"": ""1"", ""rank"": 5}, ""positive"": {""suggestion"": ""the human assessment process"", ""k"": ""1"", ""rank"": 6}}",checked,3
1-1428_21171440-4bd4-49e3-846c-09f6b89703f9,annotator1,0ff39183-ed81-403d-b6ed-fc158fe2227f,"Large language models (LLMs) may suffer from spurious correlations between input texts and output labels, which limits their ability to reason based purely on general language understanding. This issue can lead to unexpected performance degradation, as LLMs may rely too heavily on individual components of input rather than integrating both premise and hypothesis for inference.","In this context, what would be a good source of inspiration for **Task calibration, a zero-shot and inference-only calibration method**?",mutual information,"{""positive"": {""suggestion"": ""mutual information"", ""k"": ""1"", ""rank"": 1}, ""random"": {""suggestion"": ""in-context learning in natural language processing"", ""k"": ""1"", ""rank"": 2}, ""sciIE"": {""suggestion"": ""zero-shot prompting of large language models (LLMs)"", ""k"": ""1"", ""rank"": 3}, ""mpnet_zero"": {""suggestion"": ""existing calibration methods for large language models"", ""k"": ""1"", ""rank"": 4}, ""gpt-4o"": {""suggestion"": ""Bayesian inference techniques"", ""k"": ""1"", ""rank"": 5}, ""ours"": {""suggestion"": ""philosophical accounts on Inference to the Best Explanation"", ""k"": ""1"", ""rank"": 6}}",checked,3
1-37477_965f8f3c-3de0-4230-ae87-1c1c5483ca32,annotator1,0ff39183-ed81-403d-b6ed-fc158fe2227f,"The study addresses the challenge of factual inaccuracies in the output of large language models, which can undermine the reliability of their responses. It highlights the need for effective methods to improve the retrieval of relevant factual information to enhance the accuracy of these models' answers.","In this context, what would be a good source of inspiration for **For the indexing and retrieval of factual information**?",the cognitive linguistic theory of frame semantics,"{""ours"": {""suggestion"": ""the bag-of-words assumption in information retrieval"", ""k"": ""1"", ""rank"": 4}, ""mpnet_zero"": {""suggestion"": ""general fact-checking capabilities of pre-trained language models"", ""k"": ""1"", ""rank"": 3}, ""random"": {""suggestion"": ""Visual in-context learning"", ""k"": ""1"", ""rank"": 6}, ""gpt-4o"": {""suggestion"": ""search engine algorithms"", ""k"": ""1"", ""rank"": 2}, ""positive"": {""suggestion"": ""the cognitive linguistic theory of frame semantics"", ""k"": ""1"", ""rank"": 1}, ""sciIE"": {""suggestion"": ""large-scale language models (LLMs)"", ""k"": ""1"", ""rank"": 5}}",,3
1-30868_92eac6d1-ca2e-40a8-9b84-255e46cb5372,annotator1,d85abaf6-a2e0-4f4b-8ca8-130127002e96,"Automated Long Answer Grading (ALAG) presents unique challenges due to the complexity and multifaceted nature of fact-based long answers, which differ significantly from shorter answer formats. The traditional score-based approaches fail to capture the nuances of student responses, highlighting the need for a more effective method to evaluate long answers in educational settings.","In this context, what would be a good source of inspiration for **Automated long answer grading**?",a rubric entailment problem,"{""positive"": {""suggestion"": ""a rubric entailment problem"", ""k"": ""1"", ""rank"": 2}, ""random"": {""suggestion"": ""the flexibility of the Choquet integral"", ""k"": ""1"", ""rank"": 6}, ""sciIE"": {""suggestion"": ""Automated Long Answer Grading (ALAG)"", ""k"": ""1"", ""rank"": 4}, ""gpt-4o"": {""suggestion"": ""natural language processing techniques"", ""k"": ""1"", ""rank"": 5}, ""ours"": {""suggestion"": ""evidence-centered design in educational assessments"", ""k"": ""1"", ""rank"": 1}, ""mpnet_zero"": {""suggestion"": ""automated essay scoring"", ""k"": ""1"", ""rank"": 3}}",,4
1-25266_18243071-a3c6-486e-a546-5cd82af2b259,annotator1,d85abaf6-a2e0-4f4b-8ca8-130127002e96,"Enhancing the reasoning capabilities of large language models (LLMs) is a key challenge, particularly for tasks requiring complex, multi-step decision-making. Existing approaches that rely solely on Chain-of-Thought reasoning do not adequately address the need for accurate world state predictions and the exploration of diverse potential actions in complex reasoning tasks.","In this context, what would be a good source of inspiration for **A novel multi-step reasoning framework for large language models**?",Humans excel at these tasks by leveraging deliberate planning with an internal world model to simulate the potential outcomes of various actions,"{""positive"": {""suggestion"": ""Humans excel at these tasks by leveraging deliberate planning with an internal world model to simulate the potential outcomes of various actions"", ""k"": ""1"", ""rank"": 1}, ""ours"": {""suggestion"": ""the idea of large language model Chain of Thought"", ""k"": ""1"", ""rank"": 5}, ""sciIE"": {""suggestion"": ""multi-step reasoning tasks"", ""k"": ""1"", ""rank"": 3}, ""gpt-4o"": {""suggestion"": ""cognitive architectures"", ""k"": ""1"", ""rank"": 2}, ""random"": {""suggestion"": ""second-order textures"", ""k"": ""1"", ""rank"": 6}, ""mpnet_zero"": {""suggestion"": ""multi-step reasoning of Large Language Models"", ""k"": ""1"", ""rank"": 4}}",,4
1-399_4303bb53-050b-41da-bdbd-0abe03f154ee,annotator1,d85abaf6-a2e0-4f4b-8ca8-130127002e96,"The trustworthiness of retrieval-augmented language models is compromised by hallucinations, primarily due to the conflict between contextual and parametric knowledge. There is a need to align these models to respond based solely on external evidence, minimizing the influence of parametric knowledge to enhance their reliability in real-world applications.","In this context, what would be a good source of inspiration for **Trustworthy language models**?",aligning language models with human preference,"{""positive"": {""suggestion"": ""aligning language models with human preference"", ""k"": ""1"", ""rank"": 5}, ""ours"": {""suggestion"": ""the generator-verifier approach in Large Language Models"", ""k"": ""1"", ""rank"": 1}, ""mpnet_zero"": {""suggestion"": ""improve the robustness and contextual awareness of Large Language Models"", ""k"": ""1"", ""rank"": 3}, ""sciIE"": {""suggestion"": ""retrieval-augmented language models"", ""k"": ""1"", ""rank"": 4}, ""gpt-4o"": {""suggestion"": ""fact-checking systems"", ""k"": ""1"", ""rank"": 2}, ""random"": {""suggestion"": ""a non-linear motion prior in the form of pixel-level trajectories"", ""k"": ""1"", ""rank"": 6}}",,4
1-10447_8735204b-8f8d-4e45-82d0-94b733a68b34,annotator1,d85abaf6-a2e0-4f4b-8ca8-130127002e96,"Formulating high-quality prompts for large language models (LLMs) is challenging for non-AI experts due to the scattered optimization principles and the lack of a structural design in existing prompt engineering research. This results in high learning costs and difficulties in the iterative updating of prompts, highlighting a need for more accessible and effective methods for prompt generation.","In this context, what would be a good source of inspiration for **Llms**?",structured reusable programming languages,"{""ours"": {""suggestion"": ""advanced prompt engineering"", ""k"": ""1"", ""rank"": 1}, ""gpt-4o"": {""suggestion"": ""cognitive psychology"", ""k"": ""1"", ""rank"": 2}, ""sciIE"": {""suggestion"": ""prompt-engineering-based large language models (LLMs)"", ""k"": ""1"", ""rank"": 3}, ""random"": {""suggestion"": ""by independently mapping the original spaces to a shared or relative one"", ""k"": ""1"", ""rank"": 4}, ""positive"": {""suggestion"": ""structured reusable programming languages"", ""k"": ""1"", ""rank"": 5}, ""mpnet_zero"": {""suggestion"": ""learning prompts using only text data derived from large language models"", ""k"": ""1"", ""rank"": 6}}",checked,3
1-26767_45892f68-7192-4f75-9f33-cfcbddbbb387,annotator1,d85abaf6-a2e0-4f4b-8ca8-130127002e96,"Current works on large language models tend to resolve questions without considering the varying levels of problem-solving difficulty, leading to an excessive focus on simpler items while neglecting more intricate ones. This presents a challenge in enhancing the reasoning capabilities of these models, particularly for multi-choice questions.","In this context, what would be a good source of inspiration for **Large language models**?",human beings using heuristics to first categorize tasks and then handle them separately,"{""sciIE"": {""suggestion"": ""Large-Language model (LLM)"", ""k"": ""1"", ""rank"": 1}, ""mpnet_zero"": {""suggestion"": ""the reasoning capabilities of the Large Language Model"", ""k"": ""1"", ""rank"": 2}, ""random"": {""suggestion"": ""a feature map"", ""k"": ""1"", ""rank"": 3}, ""gpt-4o"": {""suggestion"": ""human cognitive problem-solving strategies"", ""k"": ""1"", ""rank"": 4}, ""ours"": {""suggestion"": ""the human task-solving process"", ""k"": ""1"", ""rank"": 5}, ""positive"": {""suggestion"": ""human beings using heuristics to first categorize tasks and then handle them separately"", ""k"": ""1"", ""rank"": 6}}",checked,3
1-22200_eb6472d3-2f1c-4af2-919a-2fae50d1e078,annotator1,a1f18ded-e4b4-4d73-b4b8-391ed2bba500,"Current large language models (LLMs) are generally poorly calibrated and over-confident, particularly when utilizing reinforcement learning from human feedback (RLHF). Existing calibration methods focus on estimating individual confidence without leveraging the potential of collective interactions among multiple LLMs, which could enhance both accuracy and calibration.","In this context, what would be a good source of inspiration for **Existing calibration methods for large language models**?","humans, whose decisions and confidences not only stem from intrinsic beliefs but can also be adjusted through daily observations","{""sciIE"": {""suggestion"": ""fine-tuned large language models (LLMs)"", ""k"": ""1"", ""rank"": 4}, ""positive"": {""suggestion"": ""humans, whose decisions and confidences not only stem from intrinsic beliefs but can also be adjusted through daily observations"", ""k"": ""1"", ""rank"": 1}, ""gpt-4o"": {""suggestion"": ""Bayesian inference"", ""k"": ""1"", ""rank"": 3}, ""ours"": {""suggestion"": ""uncertainty estimation literature in large language models"", ""k"": ""1"", ""rank"": 2}, ""random"": {""suggestion"": ""a language-aligned feature space (eg CLIP)"", ""k"": ""1"", ""rank"": 6}, ""mpnet_zero"": {""suggestion"": ""fine-tuning large language models"", ""k"": ""1"", ""rank"": 5}}",,3
1-36819_870a3e83-e2b1-433a-ac36-5c2e2df62980,annotator1,a1f18ded-e4b4-4d73-b4b8-391ed2bba500,"The rapid dissemination of misinformation through social media highlights the need for effective automated fact-checking methods. Additionally, existing deep neural models have not yet achieved a level of reasoning that matches human capabilities, indicating a gap in current research that needs to be addressed.","In this context, what would be a good source of inspiration for **An explanation generation process of the model's veracity prediction**?",a text summarization problem,"{""positive"": {""suggestion"": ""a text summarization problem"", ""k"": ""1"", ""rank"": 3}, ""random"": {""suggestion"": ""an autoregressive Multi-modal Large Language Model"", ""k"": ""1"", ""rank"": 5}, ""sciIE"": {""suggestion"": ""semantic embeddings of disinformation"", ""k"": ""1"", ""rank"": 4}, ""gpt-4o"": {""suggestion"": ""causal reasoning frameworks"", ""k"": ""1"", ""rank"": 2}, ""ours"": {""suggestion"": ""a structural analysis of human reasoning, viewed through Heidegger's notion of truth as \""unconcealment\"" is conducted"", ""k"": ""1"", ""rank"": 1}, ""mpnet_zero"": {""suggestion"": ""Misinformation detection on social media"", ""k"": ""1"", ""rank"": 6}}",,3
1-546_2bf588d1-9f7c-411d-a7c5-fa94a4b9c781,annotator1,a1f18ded-e4b4-4d73-b4b8-391ed2bba500,"Prompt engineering is labor-intensive and often requires extensive manual effort to identify the most informative demonstrations for large language models, which can be challenging due to the need to sift through a vast search space. This highlights a need for more efficient methods to refine prompts and improve model performance through effective instruction delivery.","In this context, what would be a good source of inspiration for **Prompt engineering**?",active learning,"{""random"": {""suggestion"": ""a framework for motion planning of robotic arms"", ""k"": ""1"", ""rank"": 6}, ""gpt-4o"": {""suggestion"": ""curriculum learning"", ""k"": ""1"", ""rank"": 3}, ""positive"": {""suggestion"": ""active learning"", ""k"": ""1"", ""rank"": 2}, ""sciIE"": {""suggestion"": ""prompt-engineering-based large language models (LLMs)"", ""k"": ""1"", ""rank"": 4}, ""ours"": {""suggestion"": ""an instruction learning problem"", ""k"": ""1"", ""rank"": 1}, ""mpnet_zero"": {""suggestion"": ""instruction-based prompts widely used in pretrained language models"", ""k"": ""1"", ""rank"": 5}}",,2
1-20500_e3c5f157-c103-4c5b-82ff-81847246f591,annotator1,a1f18ded-e4b4-4d73-b4b8-391ed2bba500,"The challenge in proactive dialogue lies in steering conversations toward predetermined goals, which Large Language Models (LLMs) typically struggle with due to their reactive nature. Traditional methods to enhance dialogue planning in LLMs face efficiency issues or deliver suboptimal performance, indicating a need for improved strategies in this area.","In this context, what would be a good source of inspiration for **Enhance dialogue planning in large language models**?",the dual-process theory in psychology,"{""mpnet_zero"": {""suggestion"": ""the dialog capabilities of large language models"", ""k"": ""1"", ""rank"": 4}, ""sciIE"": {""suggestion"": ""dialogue model"", ""k"": ""1"", ""rank"": 5}, ""ours"": {""suggestion"": ""the diffusion model that generates future plans conditioned on the target goal and value"", ""k"": ""1"", ""rank"": 3}, ""gpt-4o"": {""suggestion"": ""reinforcement learning"", ""k"": ""1"", ""rank"": 2}, ""random"": {""suggestion"": ""The Deepfake-Specific Feature Guidance Module"", ""k"": ""1"", ""rank"": 6}, ""positive"": {""suggestion"": ""the dual-process theory in psychology"", ""k"": ""1"", ""rank"": 1}}",,2
1-3669_5ce02837-4c84-462d-8a88-c2f238deea7f,annotator1,a1f18ded-e4b4-4d73-b4b8-391ed2bba500,"Current methods for aligning large language models often fail to capture the unique characteristics and preferences of individual users, highlighting a gap in addressing personalized interactions. Additionally, challenges such as limited personal data, diverse preferences, and scalability requirements necessitate innovative approaches to enhance alignment with individual behavioral preferences.","In this context, what would be a good source of inspiration for **Aligning large language models**?",psychometrics,"{""positive"": {""suggestion"": ""psychometrics"", ""k"": ""1"", ""rank"": 2}, ""mpnet_zero"": {""suggestion"": ""aligning Large Language Models with human preferences"", ""k"": ""1"", ""rank"": 4}, ""gpt-4o"": {""suggestion"": ""reinforcement learning from human feedback"", ""k"": ""1"", ""rank"": 3}, ""random"": {""suggestion"": ""Italo Calvino's literature machines"", ""k"": ""1"", ""rank"": 6}, ""sciIE"": {""suggestion"": ""Large Language Model"", ""k"": ""1"", ""rank"": 5}, ""ours"": {""suggestion"": ""a diffusion personalization problem"", ""k"": ""1"", ""rank"": 1}}",,2
1-18595_817e5cbf-dd28-4b2e-af43-9178db7530f8,annotator2,8fa9c65f-4379-4431-9d82-5776404d23b4,"Interpreting machine learning outputs in normatively salient domains requires a broader analysis that extends beyond the understanding of the machine learning model itself, as these models are embedded within and shaped by social structures. This highlights the need for transparency that goes beyond traditional model interpretability, particularly in contexts where social factors influence algorithmic outcomes, such as in racially biased healthcare allocation.","In this context, what would be a good source of inspiration for **Interpreting the outputs of an opaque machine learning model**?",social philosophy,"{""gpt-4o"": {""suggestion"": ""sociotechnical systems"", ""k"": ""1"", ""rank"": 2}, ""ours"": {""suggestion"": ""extensive research from both the social science and AI communities"", ""k"": ""1"", ""rank"": 4}, ""random"": {""suggestion"": ""existing diffusion-based video generation backbones"", ""k"": ""1"", ""rank"": 6}, ""sciIE"": {""suggestion"": ""human-in-the-loop, fairness-aware model selection framework"", ""k"": ""1"", ""rank"": 1}, ""mpnet_zero"": {""suggestion"": ""the growing literature on model interpretability"", ""k"": ""1"", ""rank"": 5}, ""positive"": {""suggestion"": ""social philosophy"", ""k"": ""1"", ""rank"": 3}}",,3
1-38816_fb05415e-ea70-4297-9be8-99991b280fbd,annotator2,8fa9c65f-4379-4431-9d82-5776404d23b4,"The need for improved interpretability in learned representations is crucial, as existing methods often result in entangled information across dimensions, making analysis challenging. Additionally, there is a demand for representation learning techniques that allow for smoother and more predictable latent spaces to facilitate post-hoc analysis.","In this context, what would be a good source of inspiration for **Self-supervised representation learning**?",VAEs,"{""ours"": {""suggestion"": ""learning representations through masked reconstruction in latent space"", ""k"": ""1"", ""rank"": 3}, ""random"": {""suggestion"": ""open vocabulary understanding"", ""k"": ""1"", ""rank"": 1}, ""positive"": {""suggestion"": ""VAEs"", ""k"": ""1"", ""rank"": 2}, ""sciIE"": {""suggestion"": ""self-supervised representation learning"", ""k"": ""1"", ""rank"": 5}, ""gpt-4o"": {""suggestion"": ""contrastive learning"", ""k"": ""1"", ""rank"": 4}, ""mpnet_zero"": {""suggestion"": ""unsupervised representation learning"", ""k"": ""1"", ""rank"": 6}}",,4
1-7599_d93e817d-dabb-4601-8609-10eebdd92a95,annotator2,8fa9c65f-4379-4431-9d82-5776404d23b4,"Modern AI systems often achieve superhuman performance but lack essential human-like features such as generalization, interpretability, and human inter-operability. There is a need for AI agents to learn more interpretable and generalizable behaviors that can facilitate effective human-AI coordination.","In this context, what would be a good source of inspiration for **Ai agents**?",the rich interactions between language and decision-making in humans,"{""ours"": {""suggestion"": ""Humans can quickly learn new behaviors by leveraging background world knowledge"", ""k"": ""1"", ""rank"": 1}, ""random"": {""suggestion"": ""visual tokens"", ""k"": ""1"", ""rank"": 5}, ""sciIE"": {""suggestion"": ""AI agents"", ""k"": ""1"", ""rank"": 6}, ""mpnet_zero"": {""suggestion"": ""replicating human-like cognitive processes in AI"", ""k"": ""1"", ""rank"": 3}, ""positive"": {""suggestion"": ""the rich interactions between language and decision-making in humans"", ""k"": ""1"", ""rank"": 2}, ""gpt-4o"": {""suggestion"": ""cognitive neuroscience"", ""k"": ""1"", ""rank"": 4}}",,4
1-36865_ac6713dd-42ae-42a7-a000-69c7ba40368d,annotator2,8fa9c65f-4379-4431-9d82-5776404d23b4,"The authenticity of the cause in abductive logical reasoning remains underexplored, particularly due to challenges such as reversing thinking patterns and the presence of irrelevant premises. This highlights a need for improved methods to determine the validity of reasoning in large language models.","In this context, what would be a good source of inspiration for **A new framework for large language models abductive logical reasoning**?",hypothesis and verification method and identification of irrelevant information in human thinking process,"{""mpnet_zero"": {""suggestion"": ""enhancing the validity of reasoning processes in Large Language Models"", ""k"": ""1"", ""rank"": 5}, ""ours"": {""suggestion"": ""Miller's cognitive model of explanation"", ""k"": ""1"", ""rank"": 1}, ""gpt-4o"": {""suggestion"": ""Bayesian inference"", ""k"": ""1"", ""rank"": 3}, ""positive"": {""suggestion"": ""hypothesis and verification method and identification of irrelevant information in human thinking process"", ""k"": ""1"", ""rank"": 2}, ""sciIE"": {""suggestion"": ""abductive logical reasoning dataset"", ""k"": ""1"", ""rank"": 4}, ""random"": {""suggestion"": ""detection transformer"", ""k"": ""1"", ""rank"": 6}}",,3
1-27894_a6268477-c849-4d80-baeb-1868347869cd,annotator2,8fa9c65f-4379-4431-9d82-5776404d23b4,"Pretrained language models struggle to effectively capture sentence and document-level semantics, and transferring perturbation-based methods from visual representation learning to NLP remains an unresolved issue due to the limitations imposed by tokenization. This highlights a significant gap in existing research, necessitating innovative approaches to enhance understanding of textual semantics.","In this context, what would be a good source of inspiration for **The learning of sentence-level textual semantics**?",a visual representation learning process,"{""sciIE"": {""suggestion"": ""visual language model"", ""k"": ""1"", ""rank"": 6}, ""random"": {""suggestion"": ""Relative Position Encoding"", ""k"": ""1"", ""rank"": 1}, ""mpnet_zero"": {""suggestion"": ""the rich semantic knowledge embedded in Visual Language Models"", ""k"": ""1"", ""rank"": 2}, ""gpt-4o"": {""suggestion"": ""contextual embeddings"", ""k"": ""1"", ""rank"": 4}, ""ours"": {""suggestion"": ""a text-to-token task"", ""k"": ""1"", ""rank"": 5}, ""positive"": {""suggestion"": ""a visual representation learning process"", ""k"": ""1"", ""rank"": 3}}",,4
1-25902_c0ee2409-f0a0-487c-8a41-c0f01d8ebfc4,annotator2,8fa9c65f-4379-4431-9d82-5776404d23b4,Recent methods for knowledge-based visual question answering (K-VQA) lack interpretability as they do not explicitly show the knowledge needed to answer questions. This highlights a need for approaches that can generate and incorporate relevant knowledge in a more transparent manner.,"In this context, what would be a good source of inspiration for **A similar knowledge-generation-based k-vqa method**?",recent work on knowledge generation from LLMs for text-based QA,"{""ours"": {""suggestion"": ""a new visual prompt approach to integrate fine-grained external knowledge, gleaned from specialized vision models (e.g., instance segmentation/OCR models), into multimodal large language models"", ""k"": ""1"", ""rank"": 1}, ""sciIE"": {""suggestion"": ""visual question-answering (VQA) models"", ""k"": ""1"", ""rank"": 4}, ""positive"": {""suggestion"": ""recent work on knowledge generation from LLMs for text-based QA"", ""k"": ""1"", ""rank"": 2}, ""mpnet_zero"": {""suggestion"": ""Visual Question Answering"", ""k"": ""1"", ""rank"": 5}, ""gpt-4o"": {""suggestion"": ""neurosymbolic AI"", ""k"": ""1"", ""rank"": 3}, ""random"": {""suggestion"": ""graph representation"", ""k"": ""1"", ""rank"": 6}}",,3
1-40690_e34b7e8b-bcda-4f12-b030-8be5948a8ce2,annotator2,8fa9c65f-4379-4431-9d82-5776404d23b4,"The challenge of detecting harmful memes lies in the implicit meanings embedded within them, which are not explicitly conveyed through surface text and images. Existing harmful meme detection methods fail to provide readable explanations that reveal these implicit meanings, highlighting a significant gap in the ability to support detection decisions.","In this context, what would be a good source of inspiration for **Detect harmful memes**?",the powerful capacity of Large Language Models on text generation and reasoning,"{""sciIE"": {""suggestion"": ""in-context information of memes"", ""k"": ""1"", ""rank"": 4}, ""positive"": {""suggestion"": ""the powerful capacity of Large Language Models on text generation and reasoning"", ""k"": ""1"", ""rank"": 2}, ""random"": {""suggestion"": ""Gaussian Stochastic Weight Averaging"", ""k"": ""1"", ""rank"": 6}, ""mpnet_zero"": {""suggestion"": ""hateful meme detection"", ""k"": ""1"", ""rank"": 5}, ""ours"": {""suggestion"": ""Visual commonsense discovery in computer vision"", ""k"": ""1"", ""rank"": 1}, ""gpt-4o"": {""suggestion"": ""multimodal deep learning"", ""k"": ""1"", ""rank"": 3}}",,3
1-25077_4e3a662d-c82a-4c86-8526-17f0424157e4,annotator2,8fa9c65f-4379-4431-9d82-5776404d23b4,Achieving both accuracy and explainability in traffic prediction models remains a challenge due to the complexity of traffic data and the inherent opacity of deep learning models. Recent deep-learning architectures require intricate model designs and lack an intuitive understanding of the mapping from input data to predicted results.,"In this context, what would be a good source of inspiration for **A traffic flow prediction model based on large language models**?",transferring multi-modal traffic data into natural language descriptions,"{""sciIE"": {""suggestion"": ""traffic flow prediction"", ""k"": ""1"", ""rank"": 5}, ""gpt-4o"": {""suggestion"": ""transformer architectures"", ""k"": ""1"", ""rank"": 6}, ""mpnet_zero"": {""suggestion"": ""a deep learning-based trajectory prediction model"", ""k"": ""1"", ""rank"": 3}, ""ours"": {""suggestion"": ""the structural constraints of road network topology to guide the geographical outcomes"", ""k"": ""1"", ""rank"": 1}, ""positive"": {""suggestion"": ""transferring multi-modal traffic data into natural language descriptions"", ""k"": ""1"", ""rank"": 2}, ""random"": {""suggestion"": ""a multi-modal framework"", ""k"": ""1"", ""rank"": 4}}",,2
1-17499_eb6ad57e-a623-493c-b8c9-3df96c9dee55,annotator2,8fa9c65f-4379-4431-9d82-5776404d23b4,"Understanding the dynamics learned by spatiotemporal graph neural networks is significantly more complex than for models dealing with static data, highlighting a need for effective interpretability methods. Additionally, there is a gap in existing approaches for explaining the behavior of these models in the context of temporal graphs, particularly in identifying relevant spatial and temporal patterns for specific tasks.","In this context, what would be a good source of inspiration for **Spatiotemporal graph neural networks**?","Koopman theory, which allows a simpler description of intricate, nonlinear dynamical systems","{""positive"": {""suggestion"": ""Koopman theory, which allows a simpler description of intricate, nonlinear dynamical systems"", ""k"": ""1"", ""rank"": 1}, ""ours"": {""suggestion"": ""graph signal processing theory"", ""k"": ""1"", ""rank"": 2}, ""mpnet_zero"": {""suggestion"": ""Spatio-Temporal Graph Neural Networks"", ""k"": ""1"", ""rank"": 4}, ""gpt-4o"": {""suggestion"": ""dynamic systems theory"", ""k"": ""1"", ""rank"": 3}, ""sciIE"": {""suggestion"": ""Spatio-Temporal Graph Convolutional Network (STGCN)"", ""k"": ""1"", ""rank"": 5}, ""random"": {""suggestion"": ""influence neighbors neighbors neighbors neighbors (i.e., the ability to influence neighbors neighbors neighbors neighbors neighbors)"", ""k"": ""1"", ""rank"": 6}}",,2
1-25843_dde2d38c-a0b9-41d2-b273-d0875de0b49c,annotator2,8fa9c65f-4379-4431-9d82-5776404d23b4,"A significant limitation of current large language models is their inability to continually improve their responses, even when they are informed of their mistakes. This gap highlights the need for methods that enable models to introspect and correct their errors over multiple interactions, addressing the challenges posed by complex reasoning tasks.","In this context, what would be a good source of inspiration for **Fine-tuning large language models for introspection**?",solving a multi-turn Markov decision process,"{""gpt-4o"": {""suggestion"": ""meta-learning techniques"", ""k"": ""1"", ""rank"": 3}, ""mpnet_zero"": {""suggestion"": ""improve the performance of Large Language Models on complex reasoning and planning tasks"", ""k"": ""1"", ""rank"": 1}, ""ours"": {""suggestion"": ""the theory of \""Learning from Errors\"""", ""k"": ""1"", ""rank"": 2}, ""random"": {""suggestion"": ""a simple 3D representation suitable for robotics applications"", ""k"": ""1"", ""rank"": 6}, ""sciIE"": {""suggestion"": ""few-shot prompting of large language models"", ""k"": ""1"", ""rank"": 5}, ""positive"": {""suggestion"": ""solving a multi-turn Markov decision process"", ""k"": ""1"", ""rank"": 4}}",,4
1-42036_8b7cfcbe-070c-4783-9fb6-6b4958654aa5,annotator2,8fa9c65f-4379-4431-9d82-5776404d23b4,"Existing benchmarks for testing linguistic understanding in vision and language models do not provide insights into the internal processes these models use to arrive at their outputs, particularly regarding complex issues like negation. This gap highlights the need for a deeper analysis of model behavior to enhance our understanding of how these models process language in multimodal contexts.","In this context, what would be a good source of inspiration for **Explain the behaviour of vision \& language models on the understanding of negation**?",the growing literature on model interpretability,"{""ours"": {""suggestion"": ""cognitive science about human language processing"", ""k"": ""1"", ""rank"": 1}, ""random"": {""suggestion"": ""graph models for graphs for graphs for graphs for graphs for graphs for graphs"", ""k"": ""1"", ""rank"": 6}, ""mpnet_zero"": {""suggestion"": ""evaluating Vision-Language Models"", ""k"": ""1"", ""rank"": 4}, ""gpt-4o"": {""suggestion"": ""cognitive neuroscience of language processing"", ""k"": ""1"", ""rank"": 2}, ""positive"": {""suggestion"": ""the growing literature on model interpretability"", ""k"": ""1"", ""rank"": 3}, ""sciIE"": {""suggestion"": ""Vision language models"", ""k"": ""1"", ""rank"": 5}}",,4
1-31431_8b38a6d1-7abf-412b-8629-f8aabcf5dc4a,annotator2,8fa9c65f-4379-4431-9d82-5776404d23b4,"The functionality of large language models is closely tied to their parameters, yet there is a lack of systematic methods to explore the connections between these parameters and model performance. Additionally, models with similar structures and parameter counts can exhibit significant performance disparities, indicating a need for deeper investigations into the patterns that govern their functionality.","In this context, what would be a good source of inspiration for **Investigate llama2-7b and zephyr**?",the methods used in biological studies,"{""mpnet_zero"": {""suggestion"": ""Small language models, like Phi2, Gemma, and DistilBERT,are streamlined versions of large language models with fewer parameters that require less power and memory to run"", ""k"": ""1"", ""rank"": 1}, ""sciIE"": {""suggestion"": ""language model benchmarks"", ""k"": ""1"", ""rank"": 5}, ""gpt-4o"": {""suggestion"": ""model interpretability techniques"", ""k"": ""1"", ""rank"": 3}, ""ours"": {""suggestion"": ""investigation of the attention-based vision models"", ""k"": ""1"", ""rank"": 2}, ""positive"": {""suggestion"": ""the methods used in biological studies"", ""k"": ""1"", ""rank"": 4}, ""random"": {""suggestion"": ""fine-tuning technique using LoRA"", ""k"": ""1"", ""rank"": 6}}",,4
1-1739_1bf48796-f7dc-4311-bd54-ba58652a45b4,annotator2,8fa9c65f-4379-4431-9d82-5776404d23b4,The challenges of hand-crafted meta-structures in heterogeneous information networks (HINs) include difficulties in scaling up and a lack of focus on human comprehensibility and generalizability in previous research efforts. This highlights a need for automatic search algorithms that can effectively balance semantic explainability with empirical performance in meta-structure identification.,"In this context, what would be a good source of inspiration for **Meta-structure search**?",the emergent reasoning abilities of large language models,"{""mpnet_zero"": {""suggestion"": ""hierarchical meta networks"", ""k"": ""1"", ""rank"": 1}, ""positive"": {""suggestion"": ""the emergent reasoning abilities of large language models"", ""k"": ""1"", ""rank"": 3}, ""gpt-4o"": {""suggestion"": ""genetic algorithms"", ""k"": ""1"", ""rank"": 6}, ""random"": {""suggestion"": ""a large vision-language assistant"", ""k"": ""1"", ""rank"": 5}, ""sciIE"": {""suggestion"": ""meta-structure search framework"", ""k"": ""1"", ""rank"": 4}, ""ours"": {""suggestion"": ""philosophical accounts on Inference to the Best Explanation"", ""k"": ""1"", ""rank"": 2}}",,2
1-19241_db2fad0a-a056-4fe5-a376-42bf69abcfc2,annotator2,8fa9c65f-4379-4431-9d82-5776404d23b4,"The evaluation of large language models (LLMs) reveals significant gaps in their self-knowledge ability, which may stem from misalignment with human attention mechanisms. This highlights the need for robust evaluation frameworks to better understand the capabilities and limitations of these models.","In this context, what would be a good source of inspiration for **A self-knowledge evaluation framework**?",Feynman's principle of understanding through creation,"{""positive"": {""suggestion"": ""Feynman's principle of understanding through creation"", ""k"": ""1"", ""rank"": 1}, ""ours"": {""suggestion"": ""the evaluation mindset of human experts"", ""k"": ""1"", ""rank"": 2}, ""random"": {""suggestion"": ""Personalized PageRank scores"", ""k"": ""1"", ""rank"": 6}, ""mpnet_zero"": {""suggestion"": ""evaluation of large language models"", ""k"": ""1"", ""rank"": 4}, ""gpt-4o"": {""suggestion"": ""metacognition in cognitive science"", ""k"": ""1"", ""rank"": 3}, ""sciIE"": {""suggestion"": ""zero-shot prompting of large language models (LLMs)"", ""k"": ""1"", ""rank"": 5}}",,4
1-41947_7f30ac3f-446d-46b7-afac-fdfb821eb58b,annotator2,8fa9c65f-4379-4431-9d82-5776404d23b4,"Statutory reasoning involves applying legislative provisions to case facts, which can be complex and challenging for Natural Language Processing models. The need for increased dataset size and interpretability highlights the limitations of existing approaches in effectively addressing this task.","In this context, what would be a good source of inspiration for **Statutory reasoning**?",an analogy task,"{""positive"": {""suggestion"": ""an analogy task"", ""k"": ""1"", ""rank"": 2}, ""random"": {""suggestion"": ""federated learning paradigm"", ""k"": ""1"", ""rank"": 6}, ""ours"": {""suggestion"": ""a text reasoning problem"", ""k"": ""1"", ""rank"": 5}, ""sciIE"": {""suggestion"": ""rule-based commonsense reasoning"", ""k"": ""1"", ""rank"": 1}, ""mpnet_zero"": {""suggestion"": ""the MMLU and LegalBench datasets"", ""k"": ""1"", ""rank"": 4}, ""gpt-4o"": {""suggestion"": ""legal expert systems"", ""k"": ""1"", ""rank"": 3}}",,3
1-35299_2a0dd934-4148-44b9-8883-dea03e599e64,annotator2,8fa9c65f-4379-4431-9d82-5776404d23b4,"The inference process of existing end-to-end interpretation methods for remote sensing fine-grained ship classification is criticized for being uninterpretable, functioning as a black box model. This lack of interpretability highlights a significant challenge in accurately conveying the reasoning process behind fine-grained ship classification tasks.","In this context, what would be a good source of inspiration for **Fine-grained ship classification**?",a step-by-step reasoning task,"{""mpnet_zero"": {""suggestion"": ""remote sensing ship priors through bias terms, learned from a small trainable network"", ""k"": ""1"", ""rank"": 1}, ""ours"": {""suggestion"": ""explainable artificial intelligence(XAI)"", ""k"": ""1"", ""rank"": 6}, ""sciIE"": {""suggestion"": ""remote sensing ship priors"", ""k"": ""1"", ""rank"": 2}, ""positive"": {""suggestion"": ""a step-by-step reasoning task"", ""k"": ""1"", ""rank"": 3}, ""gpt-4o"": {""suggestion"": ""attention mechanisms"", ""k"": ""1"", ""rank"": 5}, ""random"": {""suggestion"": ""modality feature interaction"", ""k"": ""1"", ""rank"": 4}}",,2
1-21408_ce32f970-277e-49b5-9243-a078db6a63a8,annotator2,8fa9c65f-4379-4431-9d82-5776404d23b4,"Enhancing the reasoning capabilities of Large Language Models (LLMs) is crucial, as existing models face challenges in achieving consistent performance across various reasoning tasks. The need for improved methods is underscored by the limitations of current approaches, which do not effectively break down instance-level rewards into more granular signals for better learning outcomes.","In this context, what would be a good source of inspiration for **Enhancing the reasoning capabilities of large language models**?",the successful strategy employed by AlphaZero,"{""sciIE"": {""suggestion"": ""Large-Language model (LLM)"", ""k"": ""1"", ""rank"": 5}, ""random"": {""suggestion"": ""computer science"", ""k"": ""1"", ""rank"": 6}, ""gpt-4o"": {""suggestion"": ""cognitive neuroscience"", ""k"": ""1"", ""rank"": 3}, ""positive"": {""suggestion"": ""the successful strategy employed by AlphaZero"", ""k"": ""1"", ""rank"": 1}, ""mpnet_zero"": {""suggestion"": ""enhancing the validity of reasoning processes in Large Language Models"", ""k"": ""1"", ""rank"": 4}, ""ours"": {""suggestion"": ""a general reward model"", ""k"": ""1"", ""rank"": 2}}",,4
1-15477_16fa3d23-9ee0-4305-b2dd-cc94639b7243,annotator3,95072f28-b6a4-4efd-9d9a-b337df87362d,"Traditional approaches to query-focused summarization rely on the availability of relevant documents, which is not always feasible, particularly in specialized topics. This limitation highlights the need for a more adaptable method that can generate accurate summaries without depending on pre-existing document sets.","In this context, what would be a good source of inspiration for **Query-focused summarization**?",a knowledge-intensive task setup,"{""ours"": {""suggestion"": ""a text summarization problem"", ""k"": ""1"", ""rank"": 6}, ""random"": {""suggestion"": ""external working memory"", ""k"": ""1"", ""rank"": 2}, ""sciIE"": {""suggestion"": ""Query-focused summarization (QFS)"", ""k"": ""1"", ""rank"": 5}, ""positive"": {""suggestion"": ""a knowledge-intensive task setup"", ""k"": ""1"", ""rank"": 1}, ""gpt-4o"": {""suggestion"": ""neural attention mechanisms"", ""k"": ""1"", ""rank"": 3}, ""mpnet_zero"": {""suggestion"": ""literature summarization"", ""k"": ""1"", ""rank"": 4}}",,4
1-1622_1e9cce01-0ab4-4c8d-a8d7-f7463807e438,annotator3,95072f28-b6a4-4efd-9d9a-b337df87362d,"Existing extractive summarization methods often produce lists of sentences that lack cohesion, which can hinder the readability and informativeness of the summaries. Additionally, when dealing with highly redundant inputs, there is a need to balance informativeness and cohesion to enhance the quality of the extracted summaries.","In this context, what would be a good source of inspiration for **Extractive summaries**?",human memory,"{""ours"": {""suggestion"": ""a method for summarization"", ""k"": ""1"", ""rank"": 6}, ""sciIE"": {""suggestion"": ""summarization tasks"", ""k"": ""1"", ""rank"": 5}, ""random"": {""suggestion"": ""credit scoring"", ""k"": ""1"", ""rank"": 3}, ""gpt-4o"": {""suggestion"": ""cohesive discourse models"", ""k"": ""1"", ""rank"": 2}, ""positive"": {""suggestion"": ""human memory"", ""k"": ""1"", ""rank"": 1}, ""mpnet_zero"": {""suggestion"": ""extractive summarization"", ""k"": ""1"", ""rank"": 4}}",,4
1-5094_f8ede5ce-fe35-4c98-b69a-78798e0dd370,annotator3,95072f28-b6a4-4efd-9d9a-b337df87362d,Long text summarization remains challenging for large language models due to insufficient open-sourced training datasets and the high requirement of contextual details. This highlights a need for effective methodologies that can leverage existing resources to improve performance in summarization tasks.,"In this context, what would be a good source of inspiration for **Long text summarization**?",question answering,"{""positive"": {""suggestion"": ""question answering"", ""k"": ""1"", ""rank"": 2}, ""sciIE"": {""suggestion"": ""long-context summarizations"", ""k"": ""1"", ""rank"": 6}, ""gpt-4o"": {""suggestion"": ""hierarchical attention networks"", ""k"": ""1"", ""rank"": 1}, ""random"": {""suggestion"": ""Natural Language Inference models"", ""k"": ""1"", ""rank"": 4}, ""mpnet_zero"": {""suggestion"": ""a two-stage text summarization model"", ""k"": ""1"", ""rank"": 3}, ""ours"": {""suggestion"": ""a long-document understanding task"", ""k"": ""1"", ""rank"": 5}}",,4
1-33478_f6e8c4b8-d400-4980-8214-9c0a15ba7439,annotator3,95072f28-b6a4-4efd-9d9a-b337df87362d,"Meeting summarization is critical due to the prevalence of digital encounters, yet large language models struggle with maintaining relevance and avoiding hallucination. There is a need for improved methods to enhance summary quality by addressing various error types such as structural, omission, and irrelevance errors.","In this context, what would be a good source of inspiration for **Meeting summarization**?",the human review process,"{""positive"": {""suggestion"": ""the human review process"", ""k"": ""1"", ""rank"": 2}, ""gpt-4o"": {""suggestion"": ""human note-taking strategies"", ""k"": ""1"", ""rank"": 1}, ""sciIE"": {""suggestion"": ""dialogue-based meeting summarizations"", ""k"": ""1"", ""rank"": 4}, ""ours"": {""suggestion"": ""a text summarization problem"", ""k"": ""1"", ""rank"": 6}, ""random"": {""suggestion"": ""collaborative retrieval"", ""k"": ""1"", ""rank"": 3}, ""mpnet_zero"": {""suggestion"": ""a summarization model"", ""k"": ""1"", ""rank"": 5}}",,4
1-31918_0a161722-9d48-4245-b48d-db2a99f17159,annotator3,95072f28-b6a4-4efd-9d9a-b337df87362d,"Radiology report summarization is essential for patient care, necessitating the distillation of detailed findings into concise impressions. The challenge lies in normalizing key observations and simplifying complex information to enhance accessibility and accuracy, particularly for non-expert audiences.","In this context, what would be a good source of inspiration for **Radiology report summarization**?",doctor-patient interactions,"{""mpnet_zero"": {""suggestion"": ""Automatic radiology report generation"", ""k"": ""1"", ""rank"": 5}, ""positive"": {""suggestion"": ""doctor-patient interactions"", ""k"": ""1"", ""rank"": 1}, ""random"": {""suggestion"": ""Partial syntactic rules from a source domain"", ""k"": ""1"", ""rank"": 2}, ""gpt-4o"": {""suggestion"": ""natural language processing techniques"", ""k"": ""1"", ""rank"": 4}, ""ours"": {""suggestion"": ""a text summarization problem"", ""k"": ""1"", ""rank"": 6}, ""sciIE"": {""suggestion"": ""radiology report generation"", ""k"": ""1"", ""rank"": 3}}",,3
1-13002_ad56b3ab-d60a-459c-bb7a-485c9e6df2d8,annotator3,95072f28-b6a4-4efd-9d9a-b337df87362d,"Analysts often begin with an understanding of the content in a document corpus, which may include predefined categories or insights from an initial review. This necessitates a topic modeling approach that can integrate this prior knowledge while allowing for interactive engagement with the model.","In this context, what would be a good source of inspiration for **Topic modeling**?",an assignment problem,"{""mpnet_zero"": {""suggestion"": ""topic models"", ""k"": ""1"", ""rank"": 6}, ""sciIE"": {""suggestion"": ""topic modeling"", ""k"": ""1"", ""rank"": 5}, ""gpt-4o"": {""suggestion"": ""LDA (Latent Dirichlet Allocation)"", ""k"": ""1"", ""rank"": 3}, ""ours"": {""suggestion"": ""story writing"", ""k"": ""1"", ""rank"": 2}, ""positive"": {""suggestion"": ""an assignment problem"", ""k"": ""1"", ""rank"": 4}, ""random"": {""suggestion"": ""a prototype of a creative system for interactively exploring the latent space of architectural forms"", ""k"": ""1"", ""rank"": 1}}",,3
1-21425_bffab43f-4e1e-49d6-af4b-c156a6cb5801,annotator3,95072f28-b6a4-4efd-9d9a-b337df87362d,"Business Document Information Extraction (BDIE) involves transforming unstructured information into a structured format, which presents challenges in effectively extracting key information and recognizing line items. Existing methods may not adequately address the practical use cases of BDIE, highlighting the need for improved frameworks that can achieve state-of-the-art results in this domain.","In this context, what would be a good source of inspiration for **Business document information extraction**?","a Tool Use problem, where the tools are these downstream systems","{""ours"": {""suggestion"": ""a sequence-labeling task"", ""k"": ""1"", ""rank"": 2}, ""random"": {""suggestion"": ""an image classification problem"", ""k"": ""1"", ""rank"": 1}, ""sciIE"": {""suggestion"": ""Business Document Information Extraction (BDIE)"", ""k"": ""1"", ""rank"": 5}, ""mpnet_zero"": {""suggestion"": ""Information Extraction"", ""k"": ""1"", ""rank"": 4}, ""positive"": {""suggestion"": ""a Tool Use problem, where the tools are these downstream systems"", ""k"": ""1"", ""rank"": 6}, ""gpt-4o"": {""suggestion"": ""transformer models"", ""k"": ""1"", ""rank"": 3}}",,2
1-10449_08238b01-fbe5-458c-b5b3-99d0a23623a6,annotator3,95072f28-b6a4-4efd-9d9a-b337df87362d,"Extracting events from financial text presents specialized challenges due to the lengthy and discontinuous nature of events, which can hinder accurate sentiment predictions. This highlights a need for a more effective approach to event extraction that can address these complexities in financial sentiment analysis.","In this context, what would be a good source of inspiration for **Event extraction**?",a classification task,"{""gpt-4o"": {""suggestion"": ""transformer models"", ""k"": ""1"", ""rank"": 6}, ""sciIE"": {""suggestion"": ""event extraction"", ""k"": ""1"", ""rank"": 5}, ""random"": {""suggestion"": ""community detection via the Louvain algorithm"", ""k"": ""1"", ""rank"": 1}, ""positive"": {""suggestion"": ""a classification task"", ""k"": ""1"", ""rank"": 4}, ""mpnet_zero"": {""suggestion"": ""document-level event extraction"", ""k"": ""1"", ""rank"": 2}, ""ours"": {""suggestion"": ""a text summarization problem"", ""k"": ""1"", ""rank"": 3}}",,3
1-2438_2cec8acd-cab8-442a-b3c5-2c1cfdbd3112,annotator3,95072f28-b6a4-4efd-9d9a-b337df87362d,Researchers in the political and social sciences face challenges in analyzing trends in information consumption due to the impracticality of manual labeling of vast amounts of data. The need for automated scalable methods is essential to effectively detect topic-related content from millions of webpages using limited annotated data points.,"In this context, what would be a good source of inspiration for **The detection of topic-related content**?",a binary classification task,"{""ours"": {""suggestion"": ""an Extreme Multi-label Short text classification"", ""k"": ""1"", ""rank"": 1}, ""sciIE"": {""suggestion"": ""topic labelling"", ""k"": ""1"", ""rank"": 5}, ""random"": {""suggestion"": ""Chromosome abnormality detection"", ""k"": ""1"", ""rank"": 2}, ""gpt-4o"": {""suggestion"": ""transfer learning"", ""k"": ""1"", ""rank"": 4}, ""positive"": {""suggestion"": ""a binary classification task"", ""k"": ""1"", ""rank"": 3}, ""mpnet_zero"": {""suggestion"": ""topic modeling"", ""k"": ""1"", ""rank"": 6}}",,3
1-5354_99c25e0f-0e43-4381-b22e-afb3e9f3ded5,annotator3,95072f28-b6a4-4efd-9d9a-b337df87362d,"The automated detection of stakeholder roles within news content remains an underexplored domain, despite existing works focusing on salient entity extraction and political affiliations through social media data. Recognizing the various types of news stakeholders and their roles is crucial for a nuanced comprehension of news narratives.","In this context, what would be a good source of inspiration for **Stakeholder classification**?",a natural language inference task,"{""gpt-4o"": {""suggestion"": ""ontology-based classification"", ""k"": ""1"", ""rank"": 2}, ""positive"": {""suggestion"": ""a natural language inference task"", ""k"": ""1"", ""rank"": 5}, ""random"": {""suggestion"": ""a more robust and efficient control mechanism for autonomous manipulation in robot arms"", ""k"": ""1"", ""rank"": 4}, ""mpnet_zero"": {""suggestion"": ""detecting specific instances of agenda control through social media"", ""k"": ""1"", ""rank"": 1}, ""sciIE"": {""suggestion"": ""Complex news events"", ""k"": ""1"", ""rank"": 3}, ""ours"": {""suggestion"": ""a named entity recognition problem"", ""k"": ""1"", ""rank"": 6}}",,3
1-17261_3802bebd-a3b5-482a-bafa-9b82aa0cc37e,annotator3,95072f28-b6a4-4efd-9d9a-b337df87362d,"The need for dynamic, context-aware interactions in narrative applications highlights the limitations of existing frameworks that do not adapt to user and agent interactions over time. Additionally, the integration of multi-agent workflows and context summarization addresses challenges in creating engaging and responsive narrative companions.","In this context, what would be a good source of inspiration for **Create dynamic, context-aware companions that can develop over time and interact with users and each other**?",multi-agent system principles,"{""mpnet_zero"": {""suggestion"": ""Interactive digital stories"", ""k"": ""1"", ""rank"": 2}, ""random"": {""suggestion"": ""a Stackelberg competition where the machine acts as the leader and the human as the follower"", ""k"": ""1"", ""rank"": 5}, ""positive"": {""suggestion"": ""multi-agent system principles"", ""k"": ""1"", ""rank"": 6}, ""ours"": {""suggestion"": ""the awesome development of role-playing agents"", ""k"": ""1"", ""rank"": 4}, ""sciIE"": {""suggestion"": ""temporal-aware narratives"", ""k"": ""1"", ""rank"": 1}, ""gpt-4o"": {""suggestion"": ""reinforcement learning"", ""k"": ""1"", ""rank"": 3}}",,2
1-17534_a70c1448-d719-4dea-8724-5a113b26b9a7,annotator3,95072f28-b6a4-4efd-9d9a-b337df87362d,"Question Answering systems struggle with complex questions that require comprehensive information synthesis and logical coherence, which naive RAG models fail to adequately address. There is a need for a structured approach that ensures detailed coverage and produces insightful, methodically organized responses to enhance answer quality and depth.","In this context, what would be a good source of inspiration for **Question answering systems**?",systematic thinking theory,"{""random"": {""suggestion"": ""Fourier features from inputs"", ""k"": ""1"", ""rank"": 4}, ""mpnet_zero"": {""suggestion"": ""a standard question-answering task"", ""k"": ""1"", ""rank"": 5}, ""ours"": {""suggestion"": ""LLMs' Chain-of-Thought"", ""k"": ""1"", ""rank"": 2}, ""gpt-4o"": {""suggestion"": ""hierarchical knowledge graphs"", ""k"": ""1"", ""rank"": 3}, ""sciIE"": {""suggestion"": ""question-answering (QA)"", ""k"": ""1"", ""rank"": 6}, ""positive"": {""suggestion"": ""systematic thinking theory"", ""k"": ""1"", ""rank"": 1}}",,3
1-22563_23d30650-403f-4519-9c7f-b707162864ec,annotator3,95072f28-b6a4-4efd-9d9a-b337df87362d,"The exponential increase in scientific literature makes it challenging for researchers to stay current with recent advances and identify meaningful research directions. Existing methods for idea generation either trivially prompt large language models or directly expose them to extensive literature without indicating useful information, highlighting a need for more effective approaches to research ideation.","In this context, what would be a good source of inspiration for **Automating the generation of novel research ideas**?",the research process of human researchers,"{""sciIE"": {""suggestion"": ""large language model-powered research idea writing agent"", ""k"": ""1"", ""rank"": 6}, ""gpt-4o"": {""suggestion"": ""knowledge graphs"", ""k"": ""1"", ""rank"": 3}, ""mpnet_zero"": {""suggestion"": ""a ResearchAgent, a large language model-powered research idea writing agent, which automatically generates problems, methods, and experiment designs"", ""k"": ""1"", ""rank"": 4}, ""ours"": {""suggestion"": ""emulate the human process of inducing collective creativity through engaging discussions with participants from diverse backgrounds and perspectives"", ""k"": ""1"", ""rank"": 1}, ""positive"": {""suggestion"": ""the research process of human researchers"", ""k"": ""1"", ""rank"": 5}, ""random"": {""suggestion"": ""temporal information of videos"", ""k"": ""1"", ""rank"": 2}}",,3
1-3167_56672db6-52e6-46d7-aeb0-62372cb2404f,annotator3,95072f28-b6a4-4efd-9d9a-b337df87362d,"The efficacy of event representation learning is limited by the short length of event texts, which differs significantly from the text length used in the pre-training of Pre-trained Language Models (PLMs). This inconsistency in text length distribution may undermine the learning process of event representation based on PLMs, indicating a need for improved methods to enhance event comprehension capabilities.","In this context, what would be a good source of inspiration for **Event representation learning**?",prompt learning,"{""ours"": {""suggestion"": ""a variant of reading comprehension"", ""k"": ""1"", ""rank"": 2}, ""mpnet_zero"": {""suggestion"": ""contextualization from pretrained language models"", ""k"": ""1"", ""rank"": 3}, ""sciIE"": {""suggestion"": ""pre-event detection tasks (keyword extraction"", ""k"": ""1"", ""rank"": 1}, ""random"": {""suggestion"": ""a large language model as the brain"", ""k"": ""1"", ""rank"": 5}, ""gpt-4o"": {""suggestion"": ""hierarchical attention networks"", ""k"": ""1"", ""rank"": 4}, ""positive"": {""suggestion"": ""prompt learning"", ""k"": ""1"", ""rank"": 6}}",,2
1-16390_0c46000b-27db-4b08-81a6-040bee0b3883,annotator3,95072f28-b6a4-4efd-9d9a-b337df87362d,Fine-tuning large language models for specific tasks is computationally expensive and risks degrading pre-learned features. There is a need for methods that optimize task-specific performance while significantly reducing computational overhead and preventing overfitting or overwriting of existing knowledge.,"In this context, what would be a good source of inspiration for **Fine-tune pre-trained language models for specific tasks**?",the concept of controlled adjustments in physical motion,"{""sciIE"": {""suggestion"": ""large-scale pretrained language models"", ""k"": ""1"", ""rank"": 4}, ""gpt-4o"": {""suggestion"": ""parameter-efficient tuning techniques"", ""k"": ""1"", ""rank"": 2}, ""positive"": {""suggestion"": ""the concept of controlled adjustments in physical motion"", ""k"": ""1"", ""rank"": 1}, ""random"": {""suggestion"": ""image of the driver's face"", ""k"": ""1"", ""rank"": 6}, ""mpnet_zero"": {""suggestion"": ""fine-tuning of pre-trained language models"", ""k"": ""1"", ""rank"": 5}, ""ours"": {""suggestion"": ""the prevalent \""pre-training and fine-tuning\"" paradigm"", ""k"": ""1"", ""rank"": 3}}",,3
1-28650_4313e0d9-3430-4962-85a6-d59ca53132de,annotator3,95072f28-b6a4-4efd-9d9a-b337df87362d,"The increasing volume of biomedical publications and the complex nature of biomedical texts, which are primarily aimed at domain experts, present significant challenges for effective entity and relation extraction. There is a need for frameworks that can leverage external knowledge to enhance the extraction process and improve performance across various specialized biomedical topics.","In this context, what would be a good source of inspiration for **A framework for biomedical entity and relation extraction**?",how humans learn domain-specific topics,"{""positive"": {""suggestion"": ""how humans learn domain-specific topics"", ""k"": ""1"", ""rank"": 2}, ""gpt-4o"": {""suggestion"": ""knowledge graphs"", ""k"": ""1"", ""rank"": 3}, ""sciIE"": {""suggestion"": ""extraction of named entities and their relationships"", ""k"": ""1"", ""rank"": 6}, ""random"": {""suggestion"": ""multiscale time-series decomposition"", ""k"": ""1"", ""rank"": 5}, ""ours"": {""suggestion"": ""Inspired by this, we investigate the role of medical knowledge in disease diagnosis through doctor-patient interaction"", ""k"": ""1"", ""rank"": 1}, ""mpnet_zero"": {""suggestion"": ""Medical Information Extraction tasks"", ""k"": ""1"", ""rank"": 4}}",,3
1-256_ad66259b-cfb5-4d00-abff-ec5b26da0243,annotator3,95072f28-b6a4-4efd-9d9a-b337df87362d,"Automated long-form story generation often results in cohesive content that lacks engagement, highlighting a need for improved methods to enhance storytelling quality. Previous end-to-end story generation techniques have limitations that the new approach aims to address, as evidenced by its superior performance in evaluations.","In this context, what would be a good source of inspiration for **Story writing**?",a search problem,"{""ours"": {""suggestion"": ""text generation in natural language processing"", ""k"": ""1"", ""rank"": 6}, ""sciIE"": {""suggestion"": ""story generation"", ""k"": ""1"", ""rank"": 5}, ""random"": {""suggestion"": ""the way humans perceive risks during driving"", ""k"": ""1"", ""rank"": 3}, ""mpnet_zero"": {""suggestion"": ""story analysis and generation systems"", ""k"": ""1"", ""rank"": 2}, ""positive"": {""suggestion"": ""a search problem"", ""k"": ""1"", ""rank"": 4}, ""gpt-4o"": {""suggestion"": ""narrative structures"", ""k"": ""1"", ""rank"": 1}}",,3